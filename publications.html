<!DOCTYPE html>
<html lang="en" style="cursor: auto;"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<title>Chuhua Xian's Homepage</title>
<meta name="author" content="Chuhua Xian">
<meta name="copyright" content="Chuhua Xian&#39;s Homepage">
<meta name="keywords" content="Chuhua Xian, Computer Graphics, Research">

<script async="" src="./publications_files/analytics.js"></script><script src="./publications_files/Lv21bWx_TlgXJs_ZpIrXfYxh8SE.js"></script>
<script src="./publications_files/ab6emSAvjKJLfKS7Yxc1Ll67N0k.js"></script>
<link rel="stylesheet" type="text/css" href="./publications_files/style2.css">
<script src="./publications_files/jquery-1.11.3.min.js"></script>
<script src="./publications_files/velocity.min.js"></script>
<script src="./publications_files/jquery.smoothState.js"></script>
<script src="./publications_files/aes.js"></script>
<script src="./publications_files/code2.js"></script>

<body class="" style="cursor: auto;">
<div id="wrap" class="animated fadeIn"><div id="pub">
<noscript>
            <div class="warning">
                <b style="color:#a00">Warning:</b> Please enable <b>Javascript</b> for full functionality of this page
            </div>
        </noscript>
<div id="wrap_header">
<header>
<div style="float:left" class="valign">
<div style="font-size:13pt" class="valign_child">
<h1>Chuhua Xian</h1>
Associate Professor
</div>
</div>
<div style="float:right; margin-right:15px" class="valign">
<div class="valign_child">
<p style="margin-bottom:5px">
<a class="">School of Computer Science and Engineering</a><br>
</p>
<p>
<a style="font-size:14pt; font-weight:bold" class="">South China University of Technology</a>
</p>
</div>
</div>
</header>
<nav>
<ul>
    <li><a id="nav_home" href="index.html" class=""><span></span>Home</a></li>
    <li><a id="nav_pub" href="publications.html" class=""><span></span>Publications</a></li>
    <li><a id="nav_teach" href="teaching.html" class=""><span></span>Teaching</a></li>
    <li><a id="nav_research" href="group.html" class=""><span></span>Group</a></li>
    <li><a id="nav_contact" href="contact.html" class=""><span></span>Contact</a></li>
    <li><a id="nav_misc" href="personal.html" class=""><span></span>Personal</a></li>
</ul>
</nav>
</div>
<!-- <div id="icon_loading" class="hidden">Loading...</div> -->
<div id="wrap0_main" class="" style="opacity: 0;"></div>
<div id="wrap1_main" class="" style="opacity: 1;">
<div class="contents">

<div class="pubtable">
<div class="title2">Selected Papers Since 2018</div>

<!-- paper list -->
<!-- begin one paper -->

<div id="Decomposition-CAG21" class="anchor"></div>
<div class="pubwrap pub_cg pub_cv">
<div class="pubimg">
<!--
<a href="HFRGBD-tase20" class="">
-->
<img src="./publications_files/VolumeDecomposition2021.png" alt="">
</a>
</div>
<div class="pubinfo">
<span>Surface Attributes Driven Volume Segmentation for 3D-Printing</span>
<div class="pub">
Xin Liu, <b>Chuhua Xian*</b>, Shuo Jin, and Guiqing Li<br>
<em>Computers & Graphics, accepted, 2021.</em>
</div>
<div class="abs">
<!--<a class="proj" href="https://github.com/chuhuaxian/HF-RGBD"><span></span>project</a>
<a class="paper" href="https://arxiv.org/abs/2002.05067"><span></span>paper</a>
<a class="dataset" href="http://www.mae.cuhk.edu.hk/~cwang/Projects/RGBDCompletionCNNDataset.zip"><span></span>dataset</a>

<a class="video" href=""><span></span>video</a>
<a class="code" href=""><span></span>supp. material</a>
<a class="abstract" id="materialgan-sa20-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
-->
</div>
</div>
</div>
<!-- end one paper-->



<div id="Cloth-cgi21" class="anchor"></div>
<div class="pubwrap pub_cg pub_cv">
<div class="pubimg">
<!--
<a href="HFRGBD-tase20" class="">
-->
<img src="./publications_files/ClothCGI21.png" alt="">
</a>
</div>
<div class="pubinfo">
<span>3D Shape-adapted Garment Generation with Sketches</span>
<div class="pub">
Yijing Chen, <b>Chuhua Xian*</b>, Shuo Jin, and Guiqing Li <br>
<em>Computer Graphics International, September 6-10, 2021, Genever, Switzerland.</em>
</div>
<div class="abs">
<!--
<a class="proj" href="./projects/RGBDCGI20.html"><span></span>project</a>
<a class="paper" href="https://link.springer.com/chapter/10.1007/978-3-030-61864-3_21"><span></span>paper</a>
<a class="slides" href="/projects/RGBDCGI20/RGBDCGI20Slides.pdf"><span></span>slides</a>

<a class="video" href=""><span></span>video</a>
<a class="code" href=""><span></span>supp. material</a>
<a class="abstract" id="materialgan-sa20-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
-->
</div>
</div>
</div>



<div id="RGBD-cadcg20" class="anchor"></div>
<div class="pubwrap pub_cg pub_cv"> 
<div class="pubimg">
<img src="./publications_files/RGBDCADCG21.png">
</a>
</div>
<div class="pubinfo">
<span>Multi-scale Fusion Network with RGB Image Features for Depth Map Completion</span>
<div class="pub">
Bolun Zheng, <b>Chuhua Xian*</b>, and Dongjiu Zhang<br>
<em>Journal of Computer-Aided Design & Computer Graphics, accepted, 2021.</em>
</div>

<!--
<div class="abs">
<a class="proj" href="./projects/deepao.html"><span></span>project</a>
<a class="paper" href="https://ieeexplore.ieee.org/document/9052668"><span></span>paper</a>
<a class="code" href="https://github.com/chuhuaxian/DeepAO"><span></span>code</a>

</div>
-->
</div>
</div>



<div id="RGBD-cadcg20" class="anchor"></div>
<div class="pubwrap pub_cg pub_cv"> 
<div class="pubimg">
<img src="./publications_files/RecommandatationCADCG21.png">
</a>
</div>
<div class="pubinfo">
<span>Automated Ornaments Layout Method for Indoor Scene based on Design History Knowledge</span>
<div class="pub">
Bolun Zheng, <b>Chuhua Xian*</b>, Guiqing Li, and Yu Yang<br>
<em>Journal of Computer-Aided Design & Computer Graphics, accepted, 2021.</em>
</div>

<!--
<div class="abs">
<a class="proj" href="./projects/deepao.html"><span></span>project</a>
<a class="paper" href="https://ieeexplore.ieee.org/document/9052668"><span></span>paper</a>
<a class="code" href="https://github.com/chuhuaxian/DeepAO"><span></span>code</a>

</div>
-->
</div>
</div>



<div id="Camera-cgi21" class="anchor"></div>
<div class="pubwrap pub_cg pub_cv">
<div class="pubimg">
<img src="./publications_files/CameraCGI2021.png">
</a>
</div>
<div class="pubinfo">
<span>Camera Focal Length from Distances in A Single Image</span>
<div class="pub">
    Yunhui Xiong, Zuxuan Lin, Guiqing Li, <b>Chuhua Xian</b>, and Changxin Peng <br>
<em>The Visual Computer, July 2021.</em>
</div>
<div class="abs">
<!-- <a class="proj" href="./projects/HandCGI20.html"><span></span>project</a> -->
<a class="paper" href="https://link.springer.com/article/10.1007/s00371-021-02233-z"><span></span>paper</a>
<!-- <a class="slides" href="/projects/HandCGI20/HandCGI20Slides.pdf"><span></span>slides</a> -->

</div>
</div>
</div>




<div id="HFRGBD-tase20" class="anchor"></div>
<div class="pubwrap pub_cg pub_cv">
<div class="pubimg">
<!--
<a href="HFRGBD-tase20" class="">
-->
<img src="./publications_files/TASEDepthCompletion.png" alt="">
</a>
</div>
<div class="pubinfo">
<span>Fast Generation of High Fidelity RGB-D Images by Deep-learning with Adaptive Convolution</span>
<div class="pub">
<b>Chuhua Xian</b>, Dongjiu Zhang, Chengkai Dai, and <a href="https://mewangcl.github.io/" class="">Charlie C.L. Wang</a><br>
<em>IEEE Transactions on Automation Science and Engineering, July 2021,  18(3): pp.1328-1340.</em>
</div>
<div class="abs">
<a class="proj" href="https://github.com/chuhuaxian/HF-RGBD"><span></span>project</a>
<a class="paper" href="https://arxiv.org/abs/2002.05067"><span></span>paper</a>
<a class="dataset" href="http://www.mae.cuhk.edu.hk/~cwang/Projects/RGBDCompletionCNNDataset.zip"><span></span>dataset</a>
<!--
<a class="video" href=""><span></span>video</a>
<a class="code" href=""><span></span>supp. material</a>
<a class="abstract" id="materialgan-sa20-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
-->
</div>
</div>
</div>
<!-- end one paper-->



<div id="HFRGBD-fusionSR20" class="anchor"></div>
<div class="pubwrap pub_cg pub_cv">
<div class="pubimg">
<!--
<a href="HFRGBD-tase20" class="">
-->
<img src="./publications_files/FusionSR.png" alt="">
</a>
</div>
<div class="pubinfo">
<span>Multi-Scale Progressive Fusion Learning for Depth Map Super-Resolution </span>
<div class="pub">
<b>Chuhua Xian</b>, Kun Qian, Zitian Zhang, and <a href="https://mewangcl.github.io/" class="">Charlie C.L. Wang</a><br>
<em>Preprinted in arXiv, 2020.</em>
</div>
<div class="abs">
<a class="paper" href="https://arxiv.org/abs/2011.11865"><span></span>paper</a>
<!--
<a class="video" href=""><span></span>video</a>
<a class="code" href=""><span></span>supp. material</a>
<a class="abstract" id="materialgan-sa20-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
-->
</div>
</div>
</div>
<!-- end one paper-->


<div id="HAND-cgi20" class="anchor"></div>
<div class="pubwrap pub_cg pub_cv">
<div class="pubimg">
<img src="./publications_files/HANDCGI2020.png">
</a>
</div>
<div class="pubinfo">
<span>3D Hand Mesh Reconstruction from a Monocular RGB Image</span>
<div class="pub">
Hao Peng, <b>Chuhua Xian*</b>, and <a href="https://www.willyunbozhang.com/" class="">Yunbo Zhang</a><br>
<em>The Visual Computer, 2020, volume 36: pages 2227–2239.</em>
</div>
<div class="abs">
<a class="proj" href="./projects/HandCGI20.html"><span></span>project</a>
<a class="paper" href="https://link.springer.com/article/10.1007/s00371-020-01908-3"><span></span>paper</a>
<a class="slides" href="/projects/HandCGI20/HandCGI20Slides.pdf"><span></span>slides</a>

</div>
</div>
</div>


<div id="RGBD-cgi20" class="anchor"></div>
<div class="pubwrap pub_cg pub_cv">
<div class="pubimg">
<!--
<a href="HFRGBD-tase20" class="">
-->
<img src="./publications_files/CGI2020.png" alt="">
</a>
</div>
<div class="pubinfo">
<span>Elimination of Incorrect Depth Points for Depth Completion</span>
<div class="pub">
<b>Chuhua Xian*</b>, Kun Qian, Guoliang Luo, and Guiqing Li <br>
<em>Computer Graphics International, October 20-23, 2020, Genever, Switzerland.</em>
</div>
<div class="abs">
<a class="proj" href="./projects/RGBDCGI20.html"><span></span>project</a>
<a class="paper" href="https://link.springer.com/chapter/10.1007/978-3-030-61864-3_21"><span></span>paper</a>
<a class="slides" href="/projects/RGBDCGI20/RGBDCGI20Slides.pdf"><span></span>slides</a>
<!--
<a class="video" href=""><span></span>video</a>
<a class="code" href=""><span></span>supp. material</a>
<a class="abstract" id="materialgan-sa20-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
-->
</div>
</div>
</div>


<div id="POP-cag20" class="anchor"></div>
<div class="pubwrap pub_cg pub_cv">
<div class="pubimg">
<!--
<a href="HFRGBD-tase20" class="">
-->
<img src="./publications_files/cagpop2020.png" alt="">
</a>
</div>
<div class="pubinfo">
<span>Extracting POP: Pairwise orthogonal planes from point cloud using RANSAC</span>
<div class="pub">
  You Wu, Guiqing, Li, <b>Chuhua Xian</b>, Xiaofeng Ding, and YunhuiXiong<br>
<em>Computers & Graphics, 2020, accepted. </em>
</div>
<div class="abs">
<!--2021, volume 94: pages 43-51.
<a class="proj" href="./projects/RGBDCGI20.html"><span></span>project</a>
-->
<a class="paper" href="https://www.sciencedirect.com/science/article/abs/pii/S0097849320301539"><span></span>paper</a>

<!--
<a class="slides" href="/projects/RGBDCGI20/RGBDCGI20Slides.pdf"><span></span>slides</a>
<a class="video" href=""><span></span>video</a>
<a class="code" href=""><span></span>supp. material</a>
<a class="abstract" id="materialgan-sa20-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
-->
</div>
</div>
</div>



<div id="HAO-cavw20" class="anchor"></div>
<div class="pubwrap pub_cg pub_cv">
<div class="pubimg">
<!--
<a href="HFRGBD-tase20" class="">
-->
<img src="./publications_files/haocavw2020.png" alt="">
</a>
</div>
<div class="pubinfo">
<span>HAO-CNN: Filament-aware hair reconstruction based on volumetric vector fields</span>
<div class="pub">
    Zehao Ye, Guiqing Li, Biyuan Yao, and <b>Chuhua Xian</b><br>
<em>Computer Animation & Vitual World, 2020, volume 31.</em>
</div>
<div class="abs">
<!--
<a class="proj" href="./projects/RGBDCGI20.html"><span></span>project</a>
-->
<a class="paper" href="https://onlinelibrary.wiley.com/doi/epdf/10.1002/cav.1945"><span></span>paper</a>

<!--
<a class="slides" href="/projects/RGBDCGI20/RGBDCGI20Slides.pdf"><span></span>slides</a>
<a class="video" href=""><span></span>video</a>
<a class="code" href=""><span></span>supp. material</a>
<a class="abstract" id="materialgan-sa20-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
-->
</div>
</div>
</div>



<div id="DeepAO-access20" class="anchor"></div>
<div class="pubwrap pub_cg pub_cv"> 
<div class="pubimg">
<img src="./publications_files/AO2020.png">
</a>
</div>
<div class="pubinfo">
<span>DeepAO: Efficient Screen Space Ambient Occlusion Generation via Deep Network </span>
<div class="pub">
Dongjiu Zhang, <b>Chuhua Xian*</b>, Guoliang Luo, Yunhui Xiong and Chu Han<br>
<em>IEEE Access, 2020, volume 8: pages 64434-64441.</em>
</div>
<div class="abs">
<a class="proj" href="./projects/deepao.html"><span></span>project</a>
<a class="paper" href="https://ieeexplore.ieee.org/document/9052668"><span></span>paper</a>
<a class="code" href="https://github.com/chuhuaxian/DeepAO"><span></span>code</a>

</div>
</div>
</div>



<div id="Mesh-cadcg20" class="anchor"></div>
<div class="pubwrap pub_cg pub_cv"> 
<div class="pubimg">
<img src="./publications_files/cadcgmesh2020.png">
</a>
</div>
<div class="pubinfo">
<span>Mesh Simplification with Adaptive Simplified Ratio Based on Local Region Features </span>
<div class="pub">
Yu Yang, <b>Chuhua Xian*</b>, and Guiqing Li<br>
<em>Journal of Computer-Aided Design & Computer Graphics, 2020, 32(6): pages 857-864.</em>
</div>

<!--
<div class="abs">
<a class="proj" href="./projects/deepao.html"><span></span>project</a>
<a class="paper" href="https://ieeexplore.ieee.org/document/9052668"><span></span>paper</a>
<a class="code" href="https://github.com/chuhuaxian/DeepAO"><span></span>code</a>

</div>
-->
</div>
</div>


<div id="Deformation-iag20" class="anchor"></div>
<div class="pubwrap pub_cg pub_cv"> 
<div class="pubimg">
<img src="./publications_files/deformation2020.png">
</a>
</div>
<div class="pubinfo">
<span>Efficient 3D animation shape manipulation with region smoothness preservation</span>
<div class="pub">
Yu Yang, <b>Chuhua Xian*</b>, Junxian Huang, and Guiqing Li<br>
<em>Journal of Image and Graphics, 2020, 25(5): pages 1019-1031.</em>
</div>
</div>
</div>




<div id="MeshSimplification-jcst19" class="anchor"></div>
<div class="pubwrap pub_cg pub_cv">
<div class="pubimg">
<!--
<a href="HFRGBD-tase20" class="">
-->
<img src="./publications_files/meshjcst2019.png" alt="">
</a>
</div>
<div class="pubinfo">
<span>Progressive Furniture Model Decimation with Texture Preservation</span>
<div class="pub">
    Zhiguang Pan, <b>Chuhua Xian*</b>, Shuo Jin, and Guiqing Li<br>
<em>Journal of Computer Science and Technology, 2019, 34(6): pages 1258-1268.</em>
</div>
<div class="abs">
<!--
<a class="proj" href="./projects/RGBDCGI20.html"><span></span>project</a>
-->
<a class="paper" href="https://link.springer.com/article/10.1007/s11390-019-1974-0"><span></span>paper</a>

<!--
<a class="slides" href="/projects/RGBDCGI20/RGBDCGI20Slides.pdf"><span></span>slides</a>
<a class="video" href=""><span></span>video</a>
<a class="code" href=""><span></span>supp. material</a>
<a class="abstract" id="materialgan-sa20-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
-->
</div>
</div>
</div>




<div id="Head-cag19" class="anchor"></div>
<div class="pubwrap pub_cg pub_cv">
<div class="pubimg">
<!--
<a href="HFRGBD-tase20" class="">
-->
<img src="./publications_files/headcag19.png" alt="">
</a>
</div>
<div class="pubinfo">
<span>Data-driven 3D human head reconstruction</span>
<div class="pub">
    Huayun He, Guiqing Li, Zehao Ye, <b>Chuhua Xian</b>, and Yongwei Nie<br>
<em>Computers & Graphics, 2019, volume 80: pages 85-96.</em>
</div>
<div class="abs">
<!--
<a class="proj" href="./projects/RGBDCGI20.html"><span></span>project</a>
-->
<a class="paper" href="https://www.sciencedirect.com/science/article/abs/pii/S0097849319300317"><span></span>paper</a>

<!--
<a class="slides" href="/projects/RGBDCGI20/RGBDCGI20Slides.pdf"><span></span>slides</a>
<a class="video" href=""><span></span>video</a>
<a class="code" href=""><span></span>supp. material</a>
<a class="abstract" id="materialgan-sa20-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
-->
</div>
</div>
</div>

<div id="Weighting-cga18" class="anchor"></div>
<div class="pubwrap pub_cg pub_cv"> 
<div class="pubimg">
<img src="./publications_files/WeightLBSDef.png">
</a>
</div>
<div class="pubinfo">
<span>Efficient C2-weighting for Image Warping</span>
<div class="pub">
<b>Chuhua Xian</b>,  Shuo Jin, and Charlie C.L. Wang<br>
<em> IEEE Computer Graphics and Applications, 2018, 38(1): pages 59-76.</em>
</div>
<div class="abs">
<a class="paper" href="https://mewangcl.github.io/pubs/WeightLBSDef.pdf"><span></span>paper</a>
<a class="results" href="https://mewangcl.github.io/pubs/WeightLBSDefMoreRes.pdf"><span></span>more results</a>
<a class="video" href="https://youtu.be/nLyX36uIcuE"><span></span>video</a>

</div>
</div>
</div>

<div id="Face-casa18" class="anchor"></div>
<div class="pubwrap pub_cg pub_cv">
<div class="pubimg">
<!--
<a href="HFRGBD-tase20" class="">
-->
<img src="./publications_files/CASA2018.png" alt="">
</a>
</div>
<div class="pubinfo">
<span>An Image Representation for the 3D Face Synthesis</span>
<div class="pub">
    Guoliang Luo, Wei Zeng, Wenqiang Xie, Hao-Peng Lei, and <b>Chuhua Xian</b><br>
<em>Proceedings of the 31st International Conference on Computer Animation and Social Agents, May 2018, Beijing, China.</em>
</div>
<div class="abs">

<a class="paper" href="https://dl.acm.org/doi/abs/10.1145/3205326.3205351"><span></span>paper</a>

</div>
</div>
</div>

</div>

<div class="pubtable">
<div class="title2">Papers before 2018</div>
Under construction...

</div>









<!--
<div id="materialgan-sa20-abs" class="abs2">
We address the problem of reconstructing spatially-varying BRDFs from a small set of image measurements. This is a fundamentally under-constrained problem, and previous work has relied on using various regularization priors or on capturing many images to produce plausible results. In this work, we present <em>MaterialGAN</em>, a deep generative convolutional network based on StyleGAN2, trained to synthesize realistic SVBRDF parameter maps. We show that MaterialGAN can be used as a powerful material prior in an inverse rendering framework: we optimize in its latent representation to generate material maps that match the appearance of the captured images when rendered. We demonstrate this framework on the task of reconstructing SVBRDFs from images captured under flash illumination using a hand-held mobile phone. Our method succeeds in producing plausible material maps that accurately reproduce the target images, and outperforms previous state-of-the-art material capture methods in evaluations on both synthetic and real data. Furthermore, our GAN-based latent space allows for high-level semantic material editing operations such as generating material variations and material morphing.
</div>
-->





<!-- original page contents


<div id="practical_cloth-sa20" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<a href="https://shuangz.com/projects/practical_cloth-sa20" class="">
<img src="./publications_files/practical_cloth-sa20.jpg" alt="practical_cloth-sa20">
</a>
</div>
<div class="pubinfo">
<span>A Practical Ply-Based Appearance Model of Woven Fabrics</span>
<div class="pub">
Zahra Montazeri, Søren B. Gammelmark, Shuang Zhao, and Henrik Wann Jensen<br>
<em>ACM Transactions on Graphics (<b>SIGGRAPH Asia 2020</b>), 39(6), December 2020</em>
</div>
<div class="abs">
<a class="proj" href="https://shuangz.com/projects/practical_cloth-sa20"><span></span>project</a>
<a class="paper" href="https://shuangz.com/projects/practical_cloth-sa20/practical_cloth-sa20.pdf"><span></span>paper</a>
<a class="paper" href="https://shuangz.com/projects/practical_cloth-sa20/practical_cloth-sa20-ld.pdf"><span></span>paper (reduced res.)</a>
<a class="video" href="https://vimeo.com/454106708"><span></span>video</a>
<a class="abstract" id="practical_cloth-sa20-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="practical_cloth-sa20-abs" class="abs2">
<p>
Simulating the appearance of woven fabrics is challenging due to the complex interplay of lighting between the constituent yarns and fibers. Conventional surface-based models lack the fidelity and details for producing realistic close-up renderings. Micro-appearance models, on the other hand, can produce highly detailed renderings by depicting fabrics fiber-by-fiber, but become expensive when handling large pieces of clothing. Further, neither surface-based nor micro-appearance model has not been shown in practice to match measurements of complex anisotropic reflection and transmission simultaneously.
</p>
<p>
In this paper, we introduce a practical appearance model for woven fabrics. We model the structure of a fabric at the ply level and simulate the local appearance of fibers making up each ply. Our model accounts for both reflection and transmission of light and is capable of matching physical measurements better than prior methods including fiber based techniques. Compared to existing micro-appearance models, our model is light-weight and scales to large pieces of clothing.
</p>
</div>
<div id="salvator_mundi-sg20" class="anchor"></div>
<div class="pubwrap pub_others">
<div class="pubimg">
<a href="https://www.mitpressjournals.org/doi/abs/10.1162/leon_a_01923" class="">
<img src="./publications_files/salvator_mundi-sg20.jpg" alt="salvator_mundi-sg20">
</a>
</div>
<div class="pubinfo">
<span>Inverse-Rendering-Based Analysis of the Fine Illumination Effects in Salvator Mundi</span>
<div class="pub">
Marco (Zhanhang) Liang, Shuang Zhao, and Michael T. Goodrich<br>
<em>Leonardo (<b>SIGGRAPH 2020 Art Paper</b>), 53(4), August 2020</em>
</div>
<div class="abs">
<a class="paper" href="https://shuangz.com/projects/salvator_mundi-sg20/salvator_mundi-sg20.pdf"><span></span>paper</a>
<a class="abstract" id="salvator_mundi-sg20-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="salvator_mundi-sg20-abs" class="abs2">
The painting Salvator Mundi is attributed to Leonardo da Vinci and depicts Jesus holding a transparent orb. The authors study the optical accuracy of the fine illumination effects in this painting using inverse rendering. Their experimental results provide plausible explanations for the strange glow inside the orb, the anomalies on the orb and the mysterious three white spots, supporting the optical accuracy of the orb’s rendering down to its fine-grain details.
</div>
<div id="psdr-sg20" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<a href="https://shuangz.com/projects/psdr-sg20" class="">
<img src="./publications_files/psdr-sg20.png" alt="psdr-sg20">
</a>
</div>
<div class="pubinfo">
<span>Path-Space Differentiable Rendering</span>
<div class="pub">
Cheng Zhang, Bailey Miller, Kai Yan, Ioannis Gkioulekas, and Shuang Zhao<br>
<em>ACM Transactions on Graphics (<b>SIGGRAPH 2020</b>), 39(4), July 2020</em>
</div>
<div class="abs">
<a class="proj" href="https://shuangz.com/projects/psdr-sg20"><span></span>project</a>
<a class="paper" href="https://shuangz.com/projects/psdr-sg20/psdr-sg20.pdf"><span></span>paper</a>
<a class="code" href="https://shuangz.com/projects/psdr-sg20/psdr-sg20_supp.zip"><span></span>supp. material</a>
<a class="code" href="https://shuangz.com/projects/psdr-sg20/psdr-sg20_code.zip"><span></span>code</a>
<a class="abstract" id="psdr-sg20-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="psdr-sg20-abs" class="abs2">
<p>
Physics-based differentiable rendering, the estimation of derivatives of radiometric measures with respect to arbitrary scene parameters, has a diverse array of applications from solving analysis-by-synthesis problems to training machine learning pipelines incorporating forward rendering processes. Unfortunately, general-purpose differentiable rendering remains challenging due to the lack of efficient estimators as well as the need to identify and handle complex discontinuities such as visibility boundaries. In this paper, we show how path integrals can be differentiated with respect to arbitrary differentiable changes of a scene. We provide a detailed theoretical analysis of this process and establish new differentiable rendering formulations based on the resulting differential path integrals. Our path-space differentiable rendering formulation allows the design of new Monte Carlo estimators that offer significantly better efficiency than state-of-the-art methods in handling complex geometric discontinuities and light transport phenomena such as caustics.
</p>
<p>
We validate our method by comparing our derivative estimates to those generated using the finite-difference method. To demonstrate the effectiveness of our technique, we compare inverse-rendering performance with a few state-of-the-art differentiable rendering methods.
</p>
</div>
<div id="lmc-sg20" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<a href="https://research.cs.cornell.edu/langevin-mcmc" class="">
<img src="./publications_files/lmc-sg20.png" alt="lmc-sg20">
</a>
</div>
<div class="pubinfo">
<span>Langevin Monte Carlo Rendering with Gradient-Based Adaptation</span>
<div class="pub">
Fujun Luan, Shuang Zhao, Kavita Bala, and Ioannis Gkioulekas<br>
<em>ACM Transactions on Graphics (<b>SIGGRAPH 2020</b>), 39(4), July 2020</em>
</div>
<div class="abs">
<a class="proj" href="https://research.cs.cornell.edu/langevin-mcmc"><span></span>project</a>
<a class="paper" href="https://shuangz.com/projects/lmc-sg20/lmc-sg20.pdf"><span></span>paper</a>
<a class="paper" href="https://shuangz.com/projects/lmc-sg20/lmc-sg20-ld.pdf"><span></span>paper (reduced res.)</a>
<a class="code" href="https://shuangz.com/projects/lmc-sg20/lmc-sg20_supp.zip"><span></span>supp. material</a>
<a class="code" href="https://github.com/luanfujun/Langevin-MCMC"><span></span>code</a>
<a class="abstract" id="lmc-sg20-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="lmc-sg20-abs" class="abs2">
We introduce a suite of Langevin Monte Carlo algorithms for efficient photorealistic rendering of scenes with complex light transport effects, such as caustics, interreflections, and occlusions. Our algorithms operate in pri- mary sample space, and use the Metropolis-adjusted Langevin algorithm (MALA) to generate new samples. Drawing inspiration from state-of-the-art stochastic gradient descent procedures, we combine MALA with adaptive preconditioning and momentum schemes that re-use previously-computed first-order gradients, either in an online or in a cache-driven fashion. This combination allows MALA to adapt to the local geometry of the primary sample space, without the computational overhead associated with previous Hessian-based adaptation algorithms. We use the theory of controlled Markov chain Monte Carlo to ensure that these combinations remain ergodic, and are therefore suitable for unbiased Monte Carlo rendering. Through extensive experiments, we show that our algorithms, MALA with online and cache-driven adaptation, can successfully handle complex light transport in a large variety of scenes, leading to improved performance (on average more than 3X variance reduction at equal time, and 7X for motion blur) compared to state-of-the-art Markov chain Monte Carlo rendering algorithms.
</div>
<div id="diffsh-sg20" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<img src="./publications_files/diffsh-sg20.jpg" alt="diffsh-sg20">
</div>
<div class="pubinfo">
<span>Analytic Spherical Harmonic Gradients for Real-Time Rendering with Many Polygonal Area Lights</span>
<div class="pub">
Lifan Wu, Guangyan Cai, Shuang Zhao, and Ravi Ramamoorthi<br>
<em>ACM Transactions on Graphics (<b>SIGGRAPH 2020</b>), 39(4), July 2020</em>
</div>
<div class="abs">
<a class="paper" href="https://shuangz.com/projects/diffsh-sg20/diffsh-sg20.pdf"><span></span>paper</a>
<a class="video" href="https://vimeo.com/429735943"><span></span>video</a>
<a class="code" href="https://shuangz.com/projects/diffsh-sg20/diffsh-sg20-code.zip"><span></span>code &amp; data</a>
<a class="abstract" id="diffsh-sg20-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="diffsh-sg20-abs" class="abs2">
Recent work has developed analytic formulae for spherical harmonic (SH) coefficients from uniform polygonal lights, enabling near-field area lights to be included in Precomputed Radiance Transfer (PRT) systems, and in offline rendering. However, the method is inefficient since coefficients need to be recomputed at each vertex or shading point, for each light, even though the SH coefficients vary smoothly in space. The complexity scales linearly with the number of lights, making many-light rendering difficult. In this paper, we develop a novel analytic formula for the spatial gradients of the spherical harmonic coefficients for uniform polygonal area lights. The result is a significant generalization, involving the Reynolds transport theorem to reduce the problem to a boundary integral for which we derive a new analytic formula, showing how to reduce a key term to an earlier recurrence for SH coefficients. The implementation requires only minor additions to existing code for SH coefficients. The results also hold implications for recent efforts on differentiable rendering. We show that SH gradients enable very sparse spatial sampling, followed by accurate Hermite interpolation. This enables scaling PRT to hundreds of area lights with minimal overhead and real-time frame rates. Moreover, the SH gradient formula is a new mathematical result that potentially enables many other graphics applications.
</div>
<div id="translucency-jov20" class="anchor"></div>
<div class="pubwrap pub_percp">
<div class="pubimg">
<a href="http://jov.arvojournals.org/article.aspx?articleid=2770314" class="">
<img src="./publications_files/translucency-jov20.png" alt="translucency-jov20">
</a>
</div>
<div class="pubinfo">
<span>Effect of Geometric Sharpness on Translucent Material Perception</span>
<div class="pub">
Bei Xiao, Shuang Zhao, Ioannis Gkioulekas, Wenyan Bi, and Kavita Bala<br>
<em>Journal of Vision, 20(7), July 2020</em>
</div>
<div class="abs">
<a class="paper" href="https://shuangz.com/projects/translucency-jov20/translucency-jov20.pdf"><span></span>paper</a>
<a class="code" href="https://github.com/BumbleBee0819/OnlinePsychophysicsExperiment_AsymmetricMatching"><span></span>code</a>
<a class="abstract" id="translucency-jov20-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="translucency-jov20-abs" class="abs2">
When judging optical properties of a translucent object, humans often look at sharp geometric features such as edges and thin parts. Analysis of the physics of light transport shows that these sharp geometries are necessary for scientific imaging systems to be able to accurately measure the underlying material optical properties. In this paper, we examine whether human perception of translucency is likewise affected by the presence of sharp geometry, by confounding our perceptual inferences about an object’s optical properties. We employ physically accurate simulations to create visual stimuli of translucent materials with varying shapes and optical properties under different illuminations. We then use these stimuli in psychophysical experiments, where human observers are asked to match an image of a target object by adjusting the material parameters of a match object with different geometric sharpness, lighting, and 3D geometry. We find that the level of geometric sharpness significantly affects perceived translucency by observers. These findings generalize across a few illumination conditions and object shapes. Our results suggest that the perceived translucency of an object depends on both the underlying material‘s optical parameters and 3D shape of the object. We also find that models based on image contrast cannot fully predict the perceptual results.
</div>
<div id="stocmed-egsr20" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<a class="proj" href="https://shuangz.com/projects/stocmed-egsr20">
<img src="./publications_files/stocmed-egsr20.jpg" alt="stocmed-egsr20">
</a>
</div>
<div class="pubinfo">
<span>Multi-Scale Appearance Modeling of Granular Materials with Continuously Varying Grain Properties</span>
 <div class="pub">
Cheng Zhang and Shuang Zhao<br>
<em>Eurographics Symposium on Rendering (EGSR), June 2020</em>
</div>
<div class="abs">
<a class="proj" href="https://shuangz.com/projects/stocmed-egsr20"><span></span>project</a>
<a class="paper" href="https://shuangz.com/projects/stocmed-egsr20/stocmed-egsr20.pdf"><span></span>paper</a>
<a class="paper" href="https://shuangz.com/projects/stocmed-egsr20/stocmed-egsr20-ld.pdf"><span></span>paper (reduced res.)</a>
<a class="video" href="https://vimeo.com/429728658"><span></span>video</a>
<a class="code" href="https://shuangz.com/projects/stocmed-egsr20/stocmed-egsr20-supp.pdf"><span></span>supp. material</a>
<a class="abstract" id="stocmed-egsr20-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="stocmed-egsr20-abs" class="abs2">
<p>
Many real-world materials such as sand, snow, salt, and rice are comprised of large collections of grains. Previously, multiscale rendering of granular materials requires precomputing light transport per grain and has difficulty in handling materials with continuously varying grain properties. Further, existing methods usually describe granular materials by explicitly storing individual grains, which becomes hugely data-intensive to describe large objects, or replicating small blocks of grains, which lacks the flexibility to describe materials with grains distributed in nonuniform manners.
</p>
<p>
We introduce a new method to render granular materials with continuously varying grain optical properties efficiently. This is achieved using a novel symbolic and differentiable simulation of light transport during precomputation. Additionally, we introduce a new representation to depict large-scale granular materials with complex grain distributions. After constructing a template tile as preprocessing, we adapt it at render time to generate large quantities of grains with user-specified distributions. We demonstrate the effectiveness of our techniques using a few examples with a variety of grain properties and distributions.
</p>
</div>
<div id="itn-iccp20" class="anchor"></div>
<div class="pubwrap pub_cg pub_cv">
<div class="pubimg">
<a href="http://imaging.cs.cmu.edu/inverse_transport_networks" class="">
<img src="./publications_files/itn-iccp20.jpg" alt="itn-iccp20">
</a>
</div>
<div class="pubinfo">
<span>Towards Learning-Based Inverse Subsurface Scattering</span>
<div class="pub">
Chengqian Che, Fujun Luan, Shuang Zhao, Kavita Bala, and Ioannis Gkioulekas<br>
<em>IEEE Inter­national Conf­erence on Comput­ational Photo­graphy (ICCP), April 2020</em>
</div>
<div class="abs">
<a class="proj" href="http://imaging.cs.cmu.edu/inverse_transport_networks"><span></span>project</a>
<a class="paper" href="https://shuangz.com/projects/itn-iccp20/itn-iccp20.pdf"><span></span>paper</a>
<a class="code" href="https://shuangz.com/projects/itn-iccp20/itn-iccp20-supp.pdf"><span></span>supp. material</a>
<a class="code" href="https://github.com/cmu-ci-lab/inverseTransportNetworks"><span></span>code</a>
<a class="abstract" id="itn-iccp20-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="itn-iccp20-abs" class="abs2">
Given images of translucent objects, of unknown shape and lighting, we aim to use learning to infer the optical parameters controlling subsurface scattering of light inside the objects. We introduce a new architecture, the inverse transport network (ITN), that aims to improve generalization of an encoder network to unseen scenes, by connecting it with a physically-accurate, differentiable Monte Carlo renderer capable of estimating image derivatives with respect to scattering material parameters. During training, this combination forces the encoder network to predict parameters that not only match groundtruth values, but also reproduce input images. During testing, the encoder network is used alone, without the renderer, to predict material parameters from a single input image. Drawing insights from the physics of radiative transfer, we additionally use material parameterizations that help reduce estimation errors due to ambiguities in the scattering parameter space. Finally, we augment the training loss with pixelwise weight maps that emphasize the parts of the image most informative about the underlying scattering parameters. We demonstrate that this combination allows neural networks to generalize to scenes with completely unseen geometries and illuminations better than traditional networks, with 38.06% reduced parameter error on average.
</div>


<div class="title2 pub_cg pub_cv pub_others">2019</div>
<div id="salvator_mundi-arxiv19" class="anchor"></div>


<div class="pubwrap pub_others">
<div class="pubimg">
<img src="./publications_files/salvator_mundi-arxiv19.png" alt="salvator_mundi-arxiv19">
</div>
<div class="pubinfo">
<span>On the Optical Accuracy of the Salvator Mundi</span>
<div class="pub">
Marco (Zhanhang) Liang, Michael T. Goodrich, and Shuang Zhao<br>
<em>Technical report (arXiv:1912.03416), December 2019</em>
</div>
<div class="abs">
<a class="paper" href="https://arxiv.org/abs/1912.03416"><span></span>paper</a>
<a class="abstract" id="salvator_mundi-arxiv19-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="salvator_mundi-arxiv19-abs" class="abs2">
A debate in the scientific literature has arisen regarding whether the orb depicted in Salvator Mundi, which has been attributed by some experts to Leonardo da Vinci, was rendered in a optically faithful manner or not. Some hypothesize that it was solid crystal while others hypothesize that it was hollow, with competing explanations for its apparent lack of background distortion and its three white spots. In this paper, we study the optical accuracy of the Salvator Mundi using physically based rendering, a sophisticated computer graphics tool that produces optically accurate images by simulating light transport in virtual scenes. We created a virtual model of the composition centered on the translucent orb in the subject's hand. By synthesizing images under configurations that vary illuminations and orb material properties, we tested whether it is optically possible to produce an image that renders the orb similarly to how it appears in the painting. Our experiments show that an optically accurate rendering qualitatively matching that of the painting is indeed possible using materials, light sources, and scientific knowledge available to Leonardo da Vinci circa 1500. We additionally tested alternative theories regarding the composition of the orb, such as that it was a solid calcite ball, which provide empirical evidence that such alternatives are unlikely to produce images similar to the painting, and that the orb is instead hollow.
</div>

<div id="vol2surf-tvcg19" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<img src="./publications_files/vol2surf-tvcg19.png" alt="vol2surf_tvcg19">
</div>
<div class="pubinfo">
<span>Multi-Scale Hybrid Micro-Appearance Modeling and Realtime Rendering of Thin Fabrics</span>
<div class="pub">
Chao Xu, Rui Wang, Shuang Zhao, and Hujun Bao<br>
<em>IEEE Transactions on Visualization and Computer Graphics, in press, 2019</em>
</div>
<div class="abs">
<a class="paper" href="https://shuangz.com/projects/vol2surf-tvcg19/vol2surf-tvcg19.pdf"><span></span>paper</a>
<a class="paper" href="https://shuangz.com/projects/vol2surf-tvcg19/vol2surf-tvcg19-ld.pdf"><span></span>paper (reduced res.)</a>
<a class="abstract" id="vol2surf-tvcg19-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="vol2surf-tvcg19-abs" class="abs2">
Micro-appearance models offer state-of-the-art quality for cloth renderings. Unfortunately, they usually rely on 3D volumes or fiber meshes that are not only data-intensive but also expensive to render. Traditional surface-based models, on the other hand, are light-weight and fast to render but normally lack the fidelity and details important for design and prototyping applications. We introduce a multi-scale, hybrid model to bridge this gap for thin fabrics. Our model enjoys both the compactness and speedy rendering offered by traditional surface-based models and the rich details provided by the micro-appearance models. Further, we propose a new algorithm to convert state-of-the-art micro-appearance models into our representation while qualitatively preserving the detailed appearance. We demonstrate the effectiveness of our technique by integrating it into a real-time rendering system.
</div>
<div id="dtrt-sa19" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<a href="https://shuangz.com/projects/dtrt-sa19" class="">
<img src="./publications_files/diffrender-sa19.png" alt="dtrt-sa19">
</a>
</div>
<div class="pubinfo">
<span>A Differential Theory of Radiative Transfer</span>
<div class="pub">
Cheng Zhang, Lifan Wu, Changxi Zheng, Ioannis Gkioulekas, Ravi Ramamoorthi, and Shuang Zhao<br>
<em>ACM Transactions on Graphics (<b>SIGGRAPH Asia 2019</b>), 38(6), November 2019</em>
</div>
<div class="abs">
<a class="proj" href="https://shuangz.com/projects/dtrt-sa19"><span></span>project</a>
<a class="paper" href="https://shuangz.com/projects/dtrt-sa19/diff_rendering.pdf"><span></span>paper</a>
<a class="slides" href="https://shuangz.com/projects/dtrt-sa19/diff_rendering.pptx"><span></span>talk slides</a>
<a class="code" href="https://shuangz.com/projects/dtrt-sa19/supp_material.zip"><span></span>supp. material</a>
<a class="code" href="https://github.com/uci-rendering/dtrt"><span></span>code</a>
<a class="abstract" id="dtrt-sa19-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="dtrt-sa19-abs" class="abs2">
<p>
Physics-based differentiable rendering is the task of estimating the derivatives of radiometric measures with respect to scene parameters. The ability to compute these derivatives is necessary for enabling gradient-based optimization in a diverse array of applications: from solving analysis-by-synthesis problems to training machine learning pipelines incorporating forward rendering processes. Unfortunately, physics-based differentiable rendering remains challenging, due to the complex and typically nonlinear relation between pixel intensities and scene parameters.
</p>
<p>
We introduce a differential theory of radiative transfer, which shows how individual components of the radiative transfer equation (RTE) can be differentiated with respect to arbitrary differentiable changes of a scene. Our theory encompasses the same generality as the standard RTE, allowing differentiation while accurately handling a large range of light transport phenomena such as volumetric absorption and scattering, anisotropic phase functions, and heterogeneity. To numerically estimate the derivatives given by our theory, we introduce an unbiased Monte Carlo estimator supporting arbitrary surface and volumetric configurations. Our technique differentiates path contributions symbolically and uses additional boundary integrals to capture geometric discontinuities such as visibility changes.
</p>
<p>
We validate our method by comparing our derivative estimations to those generated using the finite-difference method. Furthermore, we use a few synthetic examples inspired by real-world applications in inverse rendering, non-line-of-sight (NLOS) and biomedical imaging, and design, to demonstrate the practical usefulness of our technique.
</p>
</div>
<div id="mech-cloth-tvcg19" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<img src="./publications_files/mech_cloth_arxiv19.png" alt="mech_cloth_tvcg19">
</div>
<div class="pubinfo">
<span>Mechanics-Aware Modeling of Cloth Appearance</span>
<div class="pub">
Zahra Montazeri, Chang Xiao, Yun (Raymond) Fei, Changxi Zheng, and Shuang Zhao<br>
<em>IEEE Transactions on Visualization and Computer Graphics, in press, 2019</em>
</div>
<div class="abs">
<a class="paper" href="https://shuangz.com/projects/mechcloth-tvcg19/mechcloth-tvcg19.pdf"><span></span>paper</a>
<a class="video" href="https://youtu.be/10eD-tpFCNI"><span></span>video</a>
<a class="abstract" id="mech-cloth-tvcg19-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="mech-cloth-tvcg19-abs" class="abs2">
Micro-appearance models have brought unprecedented fidelity and details to cloth rendering. Yet, these models neglect fabric mechanics: when a piece of cloth interacts with the environment, its yarn and fiber arrangement usually changes in response to external contact and tension forces. Since subtle changes of a fabric's microstructures can greatly affect its macroscopic appearance, mechanics-driven appearance variation of fabrics has been a phenomenon that remains to be captured. We introduce a mechanics-aware model that adapts the microstructures of cloth yarns in a physics-based manner. Our technique works on two distinct physical scales: using physics-based simulations of individual yarns, we capture the rearrangement of yarn-level structures in response to external forces. These yarn structures are further enriched to obtain appearance-driving fiber-level details. The cross-scale enrichment is made practical through a new parameter fitting algorithm for simulation, an augmented procedural yarn model coupled with a custom-design regression neural network. We train the network using a dataset generated by joint simulations at both the yarn and the fiber levels. Through several examples, we demonstrate that our model is capable of synthesizing photorealistic cloth appearance in a mechanically plausible way.
</div>
<div id="multires-sg19" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<img src="./publications_files/multires-sg19.png" alt="multires_sg19">
</div>
<div class="pubinfo">
<span>Accurate Appearance Preserving Prefiltering for Rendering Displacement-Mapped Surfaces</span>
<div class="pub">
Lifan Wu, Shuang Zhao, Ling-Qi Yan, and Ravi Ramamoorthi<br>
<em>ACM Transactions on Graphics (<b>SIGGRAPH 2019</b>), 38(4), July 2019</em>
</div>
<div class="abs">

<a class="paper" href="https://shuangz.com/projects/multires-sg19/multires-sg19.pdf"><span></span>paper</a>
<a class="video" href="https://shuangz.com/projects/multires-sg19/multires-sg19.mp4"><span></span>video</a>
<a class="slides" href="https://shuangz.com/projects/multires-sg19/multires-sg19.pptx"><span></span>talk slides</a>
<a class="code" href="https://shuangz.com/projects/multires-sg19/multires-sg19-code.zip"><span></span>code &amp; data</a>
<a class="abstract" id="multires-sg19-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="multires-sg19-abs" class="abs2">
Prefiltering the reflectance of a displacement-mapped surface while preserving its overall appearance is challenging, as smoothing a displacement map causes complex changes of illumination effects such as shadowing-masking and interreflection. In this paper, we introduce a new method that prefilters displacement maps and BRDFs jointly and constructs SVBRDFs at reduced resolutions. These SVBRDFs preserve the appearance of the input models by capturing both shadowing-masking and interreflection effects. To expressour appearance-preserving SVBRDFs efficiently, we leverage a new representation that involves spatially varying NDFs and a novel scaling function that accurately captures micro-scale changes of shadowing, masking, and interreflection effects. Further, we show that the 6D scaling function can be factorized into a 2D function of surface location and a 4D function of direction. By exploiting the smoothness of these functions, we develop a simple and efficient factorization method that does not require computing the full scaling function. The resulting functions can be represented at low resolutions (e.g., 4^2 for the spatial function and 15^4 for the angular function),leading to minimal additional storage. Our method generalizes well to differ-ent types of geometries beyond Gaussian surfaces. Models prefiltered using our approach at different scales can be combined to form mipmaps, allowing accurate and anti-aliased level-of-detail (LoD) rendering.
</div>
<div class="title2 pub_cg pub_cv pub_percp">2018</div>
<div id="layered-sa18" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<a href="https://shuangz.com/projects/layered-sa18" class="">
<img src="./publications_files/layered_sa18.png" alt="layered_sa18">
</a>
</div>
<div class="pubinfo">
<span>Position-Free Monte Carlo Simulation for Arbitrary Layered BSDFs</span>
<div class="pub">
Yu Guo, Miloš Hašan, and Shuang Zhao<br>
<em>ACM Transactions on Graphics (<b>SIGGRAPH Asia 2018</b>), 37(6), November 2018</em>
</div>
<div class="abs">
<a class="proj" href="https://shuangz.com/projects/layered-sa18"><span></span>project</a>
<a class="paper" href="https://shuangz.com/projects/layered-sa18/layered-sa18.pdf"><span></span>paper</a>
<a class="paper" href="https://shuangz.com/projects/layered-sa18/layered-sa18-ld.pdf"><span></span>paper (reduced res.)</a>
<a class="video" href="https://vimeo.com/shuangz/layeredsa18"><span></span>video</a>
<a class="code" href="https://github.com/tflsguoyu/layeredbsdf"><span></span>code</a>
<a class="abstract" id="layered-sa18-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="layered-sa18-abs" class="abs2">
Real-world materials are often layered: metallic paints, biological tissues, and many more. Variation in the interface and volumetric scattering properties of the layers leads to a rich diversity of material appearances from anisotropic highlights to complex textures and relief patterns. However, simulating light-layer interactions is a challenging problem. Past analytical or numerical solutions either introduce several approximations and limitations, or rely on expensive operations on discretized BSDFs, preventing the ability to freely vary the layer properties spatially. We introduce a new unbiased layered BSDF model based on Monte Carlo simulation, whose only assumption is the layer assumption itself. Our novel position-free path formulation is fundamentally more powerful at constructing light transport paths than generic light transport algorithms applied to the special case of flat layers, since it is based on a product of solid angle instead of area measures, so does not contain the high-variance geometry terms needed in the standard formulation. We introduce two techniques for sampling the position-free path integral, a forward path tracer with next-event estimation and a full bidirectional estimator. We show a number of examples, featuring multiple layers with surface and volumetric scattering, surface and phase function anisotropy, and spatial variation in all parameters.
</div>

<div id="curveopt-tvcg18" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<img src="./publications_files/curveopt_tvcg17.png" alt="CurveOpt_tvcg18">
</div>
<div class="pubinfo">
<span>Inverse Diffusion Curves using Shape Optimization</span>
<div class="pub">
Shuang Zhao, Frédo Durand, and Changxi Zheng<br>
<em>IEEE Transactions on Visualization and Computer Graphics, 24(7), July 2018</em>
</div>
<div class="abs">
<a class="paper" href="https://shuangz.com/projects/curveopt-tvcg17/curveopt-tvcg17.pdf"><span></span>paper</a>
<a class="video" href="https://vimeo.com/shuangz/curveopt-tvcg17"><span></span>video</a>
<a class="code" href="https://shuangz.com/projects/curveopt-tvcg17/curveopt-tvcg17_supp.zip"><span></span>supp. material</a>
<a class="abstract" id="curveopt-tvcg18-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="curveopt-tvcg18-abs" class="abs2">
The inverse diffusion curve problem focuses on automatic creation of diffusion curve images that resemble user provided color fields. This problem is challenging since the 1D curves have a nonlinear and global impact on resulting color fields via a partial differential equation (PDE). We introduce a new approach complementary to previous methods by optimizing curve geometry. In particular, we propose a novel iterative algorithm based on the theory of shape derivatives. The resulting diffusion curves are clean and well-shaped, and the final image closely approximates the input. Our method provides a user-controlled parameter to regularize curve complexity, and generalizes to handle input color fields represented in a variety of formats.
</div>
<div id="translucency-vss18" class="anchor"></div>
<div class="pubwrap pub_percp">
<div class="pubimg">
<img src="./publications_files/vss18.jpg" alt="vss18">
</div>
<div class="pubinfo">
<span>Does Geometric Sharpness Affect Perception of Translucent Material?</span>
<div class="pub">
Bei Xiao, Wenyan Bi, Shuang Zhao, Ioannis Gkioulekas, and Kavita Bala<br>
<em>Vision Science Society Annual Meeting, May 2018</em>
</div>
<div class="abs">

<a class="abstract" id="translucency-vss18-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="translucency-vss18-abs" class="abs2">
<p>
When judging material properties of a translucent object, we often look at sharp geometric features such as edges. Image analysis shows edges of translucent objects exhibit distinctive light scattering profiles. Around the edges of translucent objects, there is often a rapid change of material thickness, which provides valuable information for recovering material properties. It was found that perception of 3D shape is different between opaque and translucent objects. Here, we examine whether geometry affects the perception of translucent material perception.
</p>
<p>
The images used in the experiment are computer-generated using Mitsuba physically based renderer. The shape of an object is described as 2D height fields (in which each pixel contains the amount of extrusion from the object surface to the base plane). We varied both material properties and 3D shapes of the stimuli: for the former, we used materials with varying optical densities (used by the radiative transfer model) so that the object would have different levels of ground-truth translucency; for the latter, we applied different amounts of Gaussian blur to the underlying height fields. Seven observers finished a paired-comparison experiment where they viewed a pair of images that had different ground-truth translucency and blur levels. They were asked to judge which object appeared to be more translucent. We also included control conditions where the objects in both images have the same blur levels.
</p>
<p>
We found that when there was no difference in the level of blurring between the images, observers could discriminate material properties of the two objects well (mean accuracy = 81%). However, when the two objects differ in the blur level, all observers started to make more mistakes (mean accuracy = 71%). We conclude that observers’ sensitivity to translucent appearance is affected by the sharpness of the 3D geometry of the object, thus suggesting 3D shape affects material perception for translucency.
</p>
</div>
<div class="title2 pub_cg">2017</div>
<div id="proccloth-egsr17" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<a href="http://www.cs.cornell.edu/projects/ctcloth/#proc-egsr17" class="">
<img src="./publications_files/proccloth-egsr17.png" alt="proccloth-egsr17">
</a>
</div>
<div class="pubinfo">
<span>Fiber-Level On-the-Fly Procedural Textiles</span>
<div class="pub">
Fujun Luan, Shuang Zhao, and Kavita Bala<br>
<em>Computer Graphics Forum (Eurographics Symposium on Rendering), 36(4), July 2017</em>
</div>
<div class="abs">
<a class="proj" href="http://www.cs.cornell.edu/projects/ctcloth/#proc-egsr17"><span></span>project</a>
<a class="paper" href="https://shuangz.com/projects/proccloth-egsr17/proccloth-egsr17.pdf"><span></span>paper</a>
<a class="video" href="https://vimeo.com/shuangz/proccloth-egsr17"><span></span>video</a>
<a class="slides" href="https://shuangz.com/projects/proccloth-egsr17/proccloth-egsr17-v2.pptx"><span></span>talk slides</a>
<a class="abstract" id="proccloth-egsr17-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="proccloth-egsr17-abs" class="abs2">
Procedural textile models are compact, easy to edit, and can achieve state-of-the-art realism with fiber-level details. However, these complex models generally need to be fully instantiated (aka. realized) into 3D volumes or fiber meshes and stored in memory, We introduce a novel realization-minimizing technique that enables physically based rendering of procedural textiles, without the need of full model realizations. The key ingredients of our technique are new data structures and search algorithms that look up regular and flyaway fibers on the fly, efficiently and consistently. Our technique works with compact fiber-level procedural yarn models in their exact form with no approximation imposed. In practice, our method can render very large models that are practically unrenderable using existing methods, while using considerably less memory (60–200X less) and achieving good performance.
</div>
<div id="nmfilter-egsr17" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">

<img src="./publications_files/nmfilter-egsr17.png" alt="nmfilter-egsr17">

</div>
<div class="pubinfo">
<span>Real-Time Linear BRDF MIP-Mapping</span>
<div class="pub">
Chao Xu, Rui Wang, Shuang Zhao, and Hujun Bao<br>
<em>Computer Graphics Forum (Eurographics Symposium on Rendering), 36(4), July 2017</em>
</div>
<div class="abs">
<a class="paper" href="https://shuangz.com/projects/nmfilter-egsr17/nmfilter-main.pdf"><span></span>paper</a>
<a class="paper" href="https://shuangz.com/projects/nmfilter-egsr17/nmfilter-main-ld.pdf"><span></span>paper (reduced res.)</a>
<a class="video" href="https://vimeo.com/218957543"><span></span>video</a>
<a class="abstract" id="nmfilter-egsr17-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
 </div>
</div>
</div>
<div id="nmfilter-egsr17-abs" class="abs2">
We present a new technique to jointly MIP-map BRDF and normal maps. Starting with generating an instant BRDF map, our technique builds its MIP-mapped versions based on a highly efficient algorithm that interpolates von Mises-Fisher (vMF) distributions. In our BRDF MIP-maps, each pixel stores a vMF mixture approximating the average of all BRDF lobes from the finest level. Our method is capable of jointly MIP-mapping BRDF and normal maps, even with high-frequency variations, at real-time while preserving high-quality reflectance details. Further, it is very fast, easy to implement, and requires no precomputation.
</div>
<div class="title2 pub_cg pub_scic">2016</div>
<div id="multires-sa16" class="anchor"></div>
<div class="pubwrap">
<div class="pubimg">
<a href="https://shuangz.com/projects/multires-sa16" class="">
<img src="./publications_files/multires-sa16.png" alt="MultiRes_sa16">
</a>
</div>
<div class="pubinfo">
<span>Downsampling Scattering Parameters for Rendering Anisotropic Media</span>
<div class="pub">
Shuang Zhao<sup>*</sup>, Lifan Wu<sup>*</sup>, Frédo Durand, and Ravi Ramamoorthi
<span style="font-size:12px;margin-left:10px">(* Joint first authors)</span><br>
<em>ACM Transactions on Graphics (<b>SIGGRAPH Asia 2016</b>), 35(6), November 2016</em>
</div>
<div class="abs">
<a class="proj" href="https://shuangz.com/projects/multires-sa16"><span></span>project</a>
<a class="paper" href="https://shuangz.com/projects/multires-sa16/multires-sa16.pdf"><span></span>paper</a>
<a class="video" href="https://vimeo.com/shuangz/multires-sa16"><span></span>video</a>
<a class="code" href="https://shuangz.com/projects/multires-sa16/multires-sa16_data.zip"><span></span>code &amp; data</a>
<a class="abstract" id="multires-sa16-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="multires-sa16-abs" class="abs2">
Volumetric micro-appearance models have provided remarkably high-quality renderings, but are highly data intensive and usually require tens of gigabytes in storage. When an object is viewed from a distance, the highest level of detail offered by these models is usually unnecessary, but traditional linear downsampling weakens the object's intrinsic shadowing structures and can yield poor accuracy. We introduce a joint optimization of single-scattering albedos and phase functions to accurately downsample heterogeneous and anisotropic media. Our method is built upon <em>scaled phase functions</em>, a new representation combining abledos and (standard) phase functions. We also show that modularity can be exploited to greatly reduce the amortized optimization overhead by allowing multiple synthesized models to share one set of downsampled parameters. Our optimized parameters generalize well to novel lighting and viewing configurations, and the resulting data sets offer several orders of magnitude storage savings.
</div>
<div id="tcure-mcqmc16" class="anchor"></div>
<div class="pubwrap pub_scic">
<div class="pubimg">
<img src="./publications_files/tcure-mcqmc16.png" alt="mcqmc16">
</div>
<div class="pubinfo">
<span>Towards Real-Time Monte Carlo for Biomedicine</span>
<div class="pub">
Shuang Zhao, Rong Kong, and Jerome Spanier<br>
<em>International Conference on Monte Carlo and Quasi-Monte Carlo Method in Scientific Computing, August 2016</em>
</div>
<div class="abs">
<a class="paper" href="https://link.springer.com/chapter/10.1007/978-3-319-91436-7_25"><span></span>paper</a>
<a class="abstract" id="tcure-mcqmc16-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="tcure-mcqmc16-abs" class="abs2">
Monte Carlo methods provide the "gold standard" computational technique for solving biomedical problems but their use is hindered by the slow convergence of the sample means. An exponential increase in the convergence rate can be obtained by adaptively modifying the sampling and weighting strategy employed. However, if the radiance is represented globally by a truncated expansion of basis functions, or locally by a region-wise constant or low degree polynomial, a bias is introduced by the truncation and/or the number of subregions. The sheer number of expansion coefficients or geometric subdivisions created by the biased representation then partly or entirely offsets the geometric acceleration of the convergence rate. As well, the (unknown amount of) bias is unacceptable for a gold standard numerical method. We introduce a new unbiased estimator of the solution of radiative transfer equation (RTE) that constrains the radiance to obey the transport equation. We provide numerical evidence of the superiority of this Transport-Constrained Unbiased Radiance Estimator (T-CURE) in various transport problems and indicate its promise for general heterogeneous problems.
</div>
<div id="ctcloth-sg16" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<a href="http://www.cs.cornell.edu/projects/ctcloth/#proc-sig15" class="">
<img src="./publications_files/ctcloth-sg16.png" alt="CTcloth_sg16">
</a>
</div>
<div class="pubinfo">
<span>Fitting Procedural Yarn Models for Realistic Cloth Rendering</span>
<div class="pub">
Shuang Zhao, Fujun Luan, and Kavita Bala<br>
<em>ACM Transactions on Graphics (<b>SIGGRAPH 2016</b>), 35(4), July 2016</em>
</div>
<div class="abs">
<a class="proj" href="http://www.cs.cornell.edu/projects/ctcloth/#proc-sig15"><span></span>project</a>
<a class="paper" href="https://shuangz.com/projects/procyarn-sg16/procyarn-sg16.pdf"><span></span>paper</a>
<a class="video" href="https://vimeo.com/shuangz/procyarn-sg16"><span></span>video</a>
<a class="slides" href="https://shuangz.com/projects/procyarn-sg16/procyarn-sg16.pptx"><span></span>talk slides</a>
<a class="code" href="https://shuangz.com/projects/procyarn-sg16/code_data_v2.zip"><span></span>code &amp; data</a>
<a class="abstract" id="ctcloth-sg16-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="ctcloth-sg16-abs" class="abs2">
Fabrics play a significant role in many applications in design, prototyping, and entertainment. Recent fiber-based models capture the rich visual appearance of fabrics, but are too onerous to design and edit. Yarn-based procedural models are powerful and convenient, but too regular and not realistic enough in appearance. In this paper, we introduce an automatic fitting approach to create high-quality procedural yarn models of fabrics with fiber-level details. We fit CT data to procedural models to automatically recover a full range of parameters, and augment the models with a measurement-based model of flyaway fibers. We validate our fabric models against CT measurements and photographs, and demonstrate the utility of this approach for fabric modeling and editing.
</div>
<div class="title2 pub_cg">2015</div>
<div id="ctcloth-tog15" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<a href="http://www.cs.cornell.edu/projects/ctcloth/index.htm#matching-cloth" class="">
<img src="./publications_files/ctcloth-tog15.png" alt="CTcloth_cacm14">
</a>
</div>
<div class="pubinfo">
<span>Matching Real Fabrics with Micro-Appearance Models</span>
<div class="pub">
Pramook Khungurn, Daniel Schroeder, Shuang Zhao, Kavita Bala, and Steve Marschner<br>
<em>ACM Transactions on Graphics, 35(1), December 2015</em>
</div>
<div class="abs">
<a class="proj" href="http://www.cs.cornell.edu/projects/ctcloth/#matching-cloth"><span></span>project</a>
<a class="paper" href="https://shuangz.com/projects/ctcloth-tog15/ctcloth-tog15.pdf"><span></span>paper</a>
<a class="video" href="https://shuangz.com/projects/ctcloth-tog15/ctcloth-tog15.mp4"><span></span>video</a>
<a class="abstract" id="ctcloth-tog15-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="ctcloth-tog15-abs" class="abs2">
Micro-appearance models explicitly model the interaction of light with microgeometry at the fiber scale to produce realistic appearance. To effectively match them to real fabrics, we introduce a new appearance matching framework to determine their parameters. Given a micro-appearance model and photographs of the fabric under many different lighting conditions, we optimize for parameters that best match the photographs using a method based on calculating derivatives during rendering. This highly applicable framework, we believe, is a useful research tool because it simplifies development and testing of new models.<br>&nbsp;<br>
Using the framework, we systematically compare several types of micro-appearance models. We acquired computed microtomography (micro CT) scans of several fabrics, photographed them under many viewing/illumination conditions, and matched several appearance models to this data. We compare a new fiber-based light scattering model to the previously used microflake model. We also compare representing cloth microgeometry using volumes derived directly from the micro CT data to using explicit fibers reconstructed from the volumes. From our comparisons we make the following conclusions: (1) given a fiber-based scattering model, volume- and fiber-based microgeometry representations are capable of very similar quality, and (2) using a fiber-specific scattering model is crucial to good results as it achieves considerably higher accuracy than prior work.
</div>
<div class="title2 pub_cg pub_thesis">2014</div>
<div class="pubwrap pub_thesis">
<div class="pubimg">
<a href="https://hdl.handle.net/1813/39013" class="">
<img src="./publications_files/CULogo4c.png" alt="thesis">
</a>
</div>
<div class="pubinfo">
<span>Modeling and Rendering Fabrics at Micron-Resolution</span>
<div class="pub">
Shuang Zhao<br>
<em>Department of Computer Science, Cornell University, August 2014</em>
</div>
<div class="abs">
<a class="paper" href="https://shuangz.com/download/zhaoThesis.pdf"><span></span>thesis</a>
</div>
</div>
</div>
<div id="ctcloth-cacm14" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<a href="https://cacm.acm.org/magazines/2014/11/179823-building-volumetric-appearance-models-of-fabric-using-micro-ct-imaging" class="">
<img src="./publications_files/cacm_2014_cover.png" alt="CTcloth_cacm14">
</a>
</div>
<div class="pubinfo">
<span>Building Volumetric Appearance Models of Fabric using Micro CT Imaging</span>
<div class="pub">
Shuang Zhao, Wenzel Jakob, Steve Marschner, and Kavita Bala<br>
<em>Communications of the ACM (Research Highlights), 57(11), November 2014</em>
<div class="highlight">
The <a href="https://shuangz.com/publications.htm#ctcloth-sg11" class="">original version</a> of this paper was published in <em>SIGGRAPH 2011</em>
</div>
<div style="margin-top:5px; margin-bottom:-2px">
<b>Technical Perspective:</b> <a href="https://shuangz.com/projects/ctcloth-cacm14/p97-rusinkiewicz.pdf" class="">The Intricate Dance of Fabric and Light</a> (by Szymon Rusinkiewicz)
</div>
</div>
<div class="abs">
 <a class="paper" href="https://shuangz.com/projects/ctcloth-cacm14/ctcloth-cacm14.pdf"><span></span>paper</a>
<a class="video" href="https://vimeo.com/shuangz/ctcloth-cacm14"><span></span>video</a>
<a class="abstract" id="ctcloth-cacm14-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="ctcloth-cacm14-abs" class="abs2">
Cloth is essential to our everyday lives; consequently, visualizing and rendering cloth has been an important area of research in graphics for decades. One important aspect contributing to the rich appearance of cloth is its complex 3D structure. Volumetric algorithms that model this 3D structure can correctly simulate the interaction of light with cloth to produce highly realistic images of cloth. But creating volumetric models of cloth is difficult: writing specialized procedures for each type of material is onerous, and requires significant programmer effort and intuition. Further, the resulting models look unrealistically “perfect” because they lack visually important features like naturally occurring irregularities.<br>&nbsp;<br>
This paper proposes a new approach to acquiring volume models, based on density data from X-ray computed tomography (CT) scans and appearance data from photographs under uncontrolled illumination. To model a material, a CT scan is made, yielding a scalar density volume. This 3D data has micron resolution details about the structure of cloth but lacks all optical information. So we combine this density data with a reference photograph of the cloth sample to infer its optical properties. We show that this approach can easily produce volume appearance models with extreme detail, and at larger scales the distinctive textures and highlights of a range of very different fabrics such as satin and velvet emerge automatically—all based simply on having accurate mesoscale geometry.
</div>
<div id="similarity-sg14" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<a href="http://www.cs.cornell.edu/projects/translucency/#similarity-sg14" class="">
<img src="./publications_files/similarity14.jpg" alt="similarity14">
</a>
</div>
<div class="pubinfo">
<span>High-Order Similarity Relations in Radiative Transfer</span>
<div class="pub">
Shuang Zhao, Ravi Ramamoorthi, and Kavita Bala<br>
<em>ACM Transactions on Graphics (<b>SIGGRAPH 2014</b>), 33(4), July 2014</em>
</div>
<div class="abs">
<a class="proj" href="http://www.cs.cornell.edu/projects/translucency/#similarity-sg14"><span></span>project</a>
<a class="paper" href="https://shuangz.com/projects/similarity-sg14/similarity-sg14.pdf"><span></span>paper</a>
<a class="slides" href="https://shuangz.com/projects/similarity-sg14/similarity-sg14.pptx"><span></span>talk slides</a>
<a class="code" href="https://shuangz.com/projects/similarity-sg14/similarity-sg14-code.zip"><span></span>code (matlab)</a>
<a class="abstract" id="similarity-sg14-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="similarity-sg14-abs" class="abs2">
Radiative transfer equations (RTEs) with different scattering parameters can lead to identical solution radiance fields. Similarity theory studies this effect by introducing a hierarchy of equivalence relations called "similarity relations". Unfortunately, given a set of scattering parameters, it remains unclear how to find altered ones satisfying these relations, significantly limiting the theory's practical value. This paper presents a complete exposition of similarity theory, which provides fundamental insights into the structure of the RTE's parameter space. To utilize the theory in its general high-order form, we introduce a new approach to solve for the altered parameters including the absorption and scattering coefficients as well as a fully tabulated phase function. We demonstrate the practical utility of our work using two applications: forward and inverse rendering of translucent media. Forward rendering is our main application, and we develop an algorithm exploiting similarity relations to offer "free" speedups for Monte Carlo rendering of optically dense and forward-scattering materials. For inverse rendering, we propose a proof-of-concept approach which warps the parameter space and greatly improves the efficiency of gradient descent algorithms. We believe similarity theory is important for simulating and acquiring volume-based appearance, and our approach has the potential to benefit a wide range of future applications in this area.
</div>
<div class="title2 pub_cg">2013</div>
<div id="scatterometer-sa13" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<a href="http://www.cs.cornell.edu/projects/translucency/#acquisition-sa13" class="">
<img src="./publications_files/translucent13.jpg" alt="translucent13">
</a>
</div>
<div class="pubinfo">
<span>Inverse Volume Rendering with Material Dictionaries</span>
<div class="pub">
Ioannis Gkioulekas, Shuang Zhao, Kavita Bala, Todd Zickler, and Anat Levin<br>
<em>ACM Transactions on Graphics (<b>SIGGRAPH Asia 2013</b>), 32(6), November 2013</em>
</div>
<div class="abs">
<a class="proj" href="http://www.cs.cornell.edu/projects/translucency/#acquisition-sa13"><span></span>project</a>
<a class="paper" href="https://shuangz.com/projects/scatterometer-sa13/scatterometer-sa13.pdf"><span></span>paper</a>
<a class="abstract" id="scatterometer-sa13-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="scatterometer-sa13-abs" class="abs2">
Translucent materials are ubiquitous, and simulating their appearance requires accurate physical parameters. However, physically-accurate parameters for scattering materials are difficult to acquire. We introduce an optimization framework for measuring bulk scattering properties of homogeneous materials (phase function, scattering coefficient, and absorption coefficient) that is more accurate, and more applicable to a broad range of materials. The optimization combines stochastic gradient descent with Monte Carlo rendering and a material dictionary to invert the radiative transfer equation. It offers several advantages: (1) it does not require isolating singlescattering events; (2) it allows measuring solids and liquids that are hard to dilute; (3) it returns parameters in physically-meaningful units; and (4) it does not restrict the shape of the phase function using Henyey-Greenstein or any other low-parameter model. We evaluate our approach by creating an acquisition setup that collects images of a material slab under narrow-beam RGB illumination. We validate results by measuring prescribed nano-dispersions and showing that recovered parameters match those predicted by Lorenz-Mie theory. We also provide a table of RGB scattering parameters for some common liquids and solids, which are validated by simulating color images in novel geometric configurations that match the corresponding photographs with less than 5% error.
</div>
<div id="translucent-tog13" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<a href="http://www.cs.cornell.edu/projects/translucency/#perception-tog13" class="">
<img src="./publications_files/translucent12.jpg" alt="translucent12">
</a>
</div>
<div class="pubinfo">
<span>Understanding the Role of Phase Function in Translucent Appearance</span>
<div class="pub">
Ioannis Gkioulekas, Bei Xiao, Shuang Zhao, Edward Adelson, Todd Zickler, and Kavita Bala<br>
<em>ACM Transactions on Graphics, 32(5), September 2013</em>
</div>
 <div class="abs">
<a class="proj" href="http://www.cs.cornell.edu/projects/translucency/#perception-tog13"><span></span>project</a>
<a class="paper" href="https://shuangz.com/projects/translucency-tog13/translucency-tog13.pdf"><span></span>paper</a>
<a class="abstract" id="translucent-tog13-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="translucent-tog13-abs" class="abs2">
Multiple scattering contributes critically to the characteristic translucent appearance of food, liquids, skin, and crystals; but little is known about how it is perceived by human observers. This paper explores the perception of translucency by studying the image effects of variations in one factor of multiple scattering: the phase function. We consider an expanded space of phase functions created by linear combinations of Henyey-Greenstein and von Mises-Fisher lobes, and we study this physical parameter space using computational data analysis and psychophysics.<br>&nbsp;<br>
Our study identifies a two-dimensional embedding of the physical scattering parameters in a perceptually-meaningful appearance space. Through our analysis of this space, we find uniform parameterizations of its two axes by analytical expressions of moments of the phase function, and provide an intuitive characterization of the visual effects that can be achieved at different parts of it. We show that our expansion of the space of phase functions enlarges the range of achievable translucent appearance compared to traditional single-parameter phase function models. Our findings highlight the important role phase function can have in controlling translucent appearance, and provide tools for manipulating its effect in material design applications.
</div>
<div id="mft-sg13" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<a href="http://www.cs.cornell.edu/projects/ctcloth/#mft-sg13" class="">
<img src="./publications_files/mft13.jpg" alt="mft13">
</a>
</div>
<div class="pubinfo">
<span>Modular Flux Transfer: Efficient Rendering of High-Resolution Volumes with Repeated Structures</span>
<div class="pub">
Shuang Zhao, Miloš Hašan, Ravi Ramamoorthi, and Kavita Bala<br>
<em>ACM Transactions on Graphics (<b>SIGGRAPH 2013</b>), 32(4), July 2013</em>
</div>
<div class="abs">
<a class="proj" href="http://www.cs.cornell.edu/projects/ctcloth/#mft-sg13"><span></span>project</a>
<a class="paper" href="https://shuangz.com/projects/mft-sg13/mft-sg13.pdf"><span></span>paper</a>
<a class="video" href="https://vimeo.com/shuangz/mft-sg13"><span></span>video</a>
<a class="slides" href="https://shuangz.com/projects/mft-sg13/mft-sg13.pptx"><span></span>talk slides</a>
<a class="abstract" id="mft-sg13-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="mft-sg13-abs" class="abs2">
The highest fidelity images to date of complex materials like cloth use extremely high-resolution volumetric models. However, rendering such complex volumetric media is expensive, with brute-force path tracing often the only viable solution. Fortunately, common volumetric materials (fabrics, finished wood, synthesized solid textures) are structured, with repeated patterns approximated by tiling a small number of exemplar blocks. In this paper, we introduce a precomputation-based rendering approach for such volumetric media with repeated structures based on a modular transfer formulation. We model each exemplar block as a voxel grid and precompute voxel-to-voxel, patch-to-patch, and patch-to-voxel flux transfer matrices. At render time, when blocks are tiled to produce a high-resolution volume, we accurately compute low-order scattering, with modular flux transfer used to approximate higher-order scattering. We achieve speedups of up to 12X over path tracing on extremely complex volumes, with minimal loss of quality. In addition, we demonstrate that our approach outperforms photon mapping on these materials.
</div>
<div class="title2 pub_cg pub_percp">2012</div>
<div id="ctcloth-sg12" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<a href="http://www.cs.cornell.edu/projects/ctcloth/#ctcloth-sg12" class="">
<img src="./publications_files/microct12.png" alt="CTcloth12">
</a>
</div>
<div class="pubinfo">
<span>Structure-Aware Synthesis for Predictive Woven Fabric Appearance</span>
<div class="pub">
Shuang Zhao, Wenzel Jakob, Steve Marschner, and Kavita Bala<br>
<em>ACM Transactions on Graphics (<b>SIGGRAPH 2012</b>), 31(4), July 2012</em>
<div class="highlight">
Paper figure selected as the <a href="https://shuangz.com/projects/ctcloth-sg12/tog_cover.pdf" class="">front cover</a> of the <em>SIGGRAPH 2012</em> conference proceedings
</div>
</div>
<div class="abs">
<a class="proj" href="http://www.cs.cornell.edu/projects/ctcloth/#ctcloth-sg12"><span></span>project</a>
<a class="paper" href="https://shuangz.com/projects/ctcloth-sg12/ctcloth-sg12.pdf"><span></span>paper</a>
<a class="video" href="https://vimeo.com/shuangz/ctcloth-sg12"><span></span>video</a>
<a class="slides" href="https://shuangz.com/projects/ctcloth-sg12/ctcloth-sg12-talk.pdf"><span></span>talk slides</a>
<a class="code" href="http://www.cs.cornell.edu/projects/ctcloth/data"><span></span>data</a>
<a class="abstract" id="ctcloth-sg12-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="ctcloth-sg12-abs" class="abs2">
Woven fabrics have a wide range of appearance determined by their small-scale 3D structure. Accurately modeling this structural detail can produce highly realistic renderings of fabrics and is critical for predictive rendering of fabric appearance. But building these yarn-level volumetric models is challenging. Procedural techniques are manually intensive, and fail to capture the naturally arising irregularities which contribute significantly to the overall appearance of cloth. Techniques that acquire the detailed 3D structure of real fabric samples are constrained only to model the scanned samples and cannot represent different fabric designs.<br>&nbsp;<br>
This paper presents a new approach to creating volumetric models of woven cloth, which starts with user-specified fabric designs and produces models that correctly capture the yarn-level structural details of cloth. We create a small database of volumetric exemplars by scanning fabric samples with simple weave structures. To build an output model, our method synthesizes a new volume by copying data from the exemplars at each yarn crossing to match a weave pattern that specifies the desired output structure. Our results demonstrate that our approach generalizes well to complex designs and can produce highly realistic results at both large and small scales.
</div>
<div id="translucency-vss12" class="anchor"></div>
<div class="pubwrap pub_percp">
<div class="pubimg">
<img src="./publications_files/vss12.jpg" alt="vss12">
</div>
<div class="pubinfo">
<span>Effects of Shape and Color on the Perception of Translucency</span>
<div class="pub">
Bei Xiao, Ioannis Gkioulekas, Asher Dunn, Shuang Zhao, Todd Zickler, Edward Adelson, and Kavita Bala<br>
<em>Vision Science Society Annual Meeting, May 2012</em>
</div>
<div class="abs">
<a class="abstract" style="display:inline-block" href="https://shuangz.com/projects/vss12/VSS12Abstract.pdf"><span></span>abstract</a>
</div>
</div>
</div>
<div class="title2 pub_cg">2011</div>
 <div id="tofbrdf-sa11" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<a href="http://www.cs.cornell.edu/projects/tofbrdf-sa11" class="">
<img src="./publications_files/tof_brdf.png" alt="ToF_BRDF">
</a>
</div>
<div class="pubinfo">
<span>Single View Reflectance Capture using Multiplexed Scattering and Time-of-Flight Imaging</span>
<div class="pub">
Nikhil Naik, Shuang Zhao, Andreas Velten, Ramesh Raskar, and Kavita Bala<br>
<em>ACM Transactions on Graphics (<b>SIGGRAPH Asia 2011</b>), 30(5), December 2011</em>
</div>
<div class="abs">
<a class="proj" href="http://www.cs.cornell.edu/projects/tofbrdf-sa11"><span></span>project</a>
<a class="paper" href="https://shuangz.com/projects/tofbrdf-sa11/tofbrdf-sa11.pdf"><span></span>paper</a>
<a class="video" href="https://vimeo.com/shuangz/tofbrdf-sa11"><span></span>video</a>
<a class="abstract" id="tofbrdf-sa11-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="tofbrdf-sa11-abs" class="abs2">
This paper introduces the concept of time-of-flight reflectance estimation, and demonstrates a new technique that allows a camera to rapidly acquire reflectance properties of objects from a single viewpoint, over relatively long distances and without encircling equipment. We measure material properties by indirectly illuminating an object by a laser source, and observing its reflected light indirectly using a time-of-flight camera. The configuration collectively acquires dense angular, but low spatial sampling, within a limited solid angle range -- all from a single viewpoint. Our ultra-fast imaging approach captures space-time "streak images" that can separate out different bounces of light based on path length. Entanglements rise in the streak images mixing signals from multiple paths if they have the same total path length. We show how reflectances can be recovered by solving for a linear system of equations and assuming parametric material models; fitting to lower dimensional reflectance models enables us to disentangle measurements.<br>&nbsp;<br>
We demonstrate proof-of-concept results of parametric reflectance models for homogeneous and discretized heterogeneous patches, both using simulation and experimental hardware. As compared to lengthy or highly calibrated BRDF acquisition techniques, we demonstrate a device that can rapidly, on the order of seconds, capture meaningful reflectance information. We expect hardware advances to improve the portability and speed of this device.
</div>
<div id="ctcloth-sg11" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<a href="http://www.cs.cornell.edu/projects/ctcloth/#ctcloth-sg11" class="">
<img src="./publications_files/microct_tall.png" alt="CTcloth">
</a>
</div>
<div class="pubinfo">
<span>Building Volumetric Appearance Models of Fabric using Micro CT Imaging</span>
<div class="pub">
Shuang Zhao, Wenzel Jakob, Steve Marschner, and Kavita Bala<br>
<em>ACM Transactions on Graphics (<b>SIGGRAPH 2011</b>), 30(4), July 2011</em>
<div class="highlight">
<a href="https://shuangz.com/publications.htm#ctcloth-cacm14" class="">Republished</a> as "Research Highlights" in <em>Communications of the ACM (CACM)</em>
</div>
</div>
<div class="abs">
<a class="proj" href="http://www.cs.cornell.edu/projects/ctcloth/#ctcloth-sg11"><span></span>project</a>
<a class="paper" href="https://shuangz.com/projects/ctcloth-sg11/ctcloth-sg11.pdf"><span></span>paper</a>
<a class="video" href="https://vimeo.com/shuangz/ctcloth-sg11"><span></span>video</a>
<a class="slides" href="https://shuangz.com/projects/ctcloth-sg11/ctcloth-sg11-talk.pdf"><span></span>talk slides</a> 
<a class="abstract" id="ctcloth-sg11-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="ctcloth-sg11-abs" class="abs2">
The appearance of complex, thick materials like textiles is determined by their 3D structure, and they are incompletely described by surface reflection models alone. While volume scattering can produce highly realistic images of such materials, creating the required volume density models is difficult. Procedural approaches require significant programmer effort and intuition to design specialpurpose algorithms for each material. Further, the resulting models lack the visual complexity of real materials with their naturallyarising irregularities.<br>&nbsp;<br>
This paper proposes a new approach to acquiring volume models, based on density data from X-ray computed tomography (CT) scans and appearance data from photographs under uncontrolled illumination. To model a material, a CT scan is made, resulting in a scalar density volume. This 3D data is processed to extract orientation information and remove noise. The resulting density and orientation fields are used in an appearance matching procedure to define scattering properties in the volume that, when rendered, produce images with texture statistics that match the photographs. As our results show, this approach can easily produce volume appearance models with extreme detail, and at larger scales the distinctive textures and highlights of a range of very different fabrics like satin and velvet emerge automatically -- all based simply on having accurate mesoscale geometry.
</div>
<div class="title2 pub_cg">2009</div>
<div id="shader-sa09" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<a href="http://www.cs.cornell.edu/projects/shader-sa09" class="">
<img src="./publications_files/shader.png" alt="shader">
</a>
</div>
<div class="pubinfo">
<span>Automatic Bounding of Programmable Shaders for Efficient Global Illumination</span>
<div class="pub">
Edgar Velázquez-Armendáriz, Shuang Zhao, Miloš Hašan, Bruce Walter, and Kavita Bala<br>
<em>ACM Transactions on Graphics (<b>SIGGRAPH Asia 2009</b>), 28(5), December 2009</em>
</div>
<div class="abs">
<a class="proj" href="http://www.cs.cornell.edu/projects/shader-sa09"><span></span>project</a>
<a class="paper" href="https://shuangz.com/projects/shader-sa09/SIGAsia09Shaders.pdf"><span></span>paper</a>
<a class="abstract" id="shader-sa09-absbtn" href="javascript:;" style="display: inline-block;"><span></span>abstract</a>
</div>
</div>
</div>
<div id="shader-sa09-abs" class="abs2">
This paper describes a technique to automatically adapt programmable shaders for use in physically-based rendering algorithms. Programmable shading provides great flexibility and power for creating rich local material detail, but only allows the material to be queried in one limited way: point sampling. Physically-based rendering algorithms simulate the complex global flow of light through an environment but rely on higher level information about the material properties, such as importance sampling and bounding, to intelligently solve high dimensional rendering integrals.<br>&nbsp;<br>
We propose using a compiler to automatically generate interval versions of programmable shaders that can be used to provide the higher level query functions needed by physically-based rendering without the need for user intervention or expertise. We demonstrate the use of programmable shaders in two such algorithms, multidimensional lightcuts and photon mapping, for a wide range of scenes including complex geometry, materials and lighting.
</div>
<div id="amber-sg09" class="anchor"></div>
<div class="pubwrap pub_cg">
<div class="pubimg">
<a href="https://shuangz.com/projects/amber-sg09" class="">
<img src="./publications_files/amber.png" alt="amber">
</a>
</div>

-->


<!-- footer -->
<footer>
<div style="float:left; padding-left:20px" class="container">
<div class="valign_child">
Copyright © 2020-2021 <a  style="font-weight:bold" class="">Chuhua Xian</a>

</div>
</div>
<div style="float:right; padding-right:20px" class="container">
<div class="valign_child">

</div>
</div>
</footer>
</div></div>
<div class="xl-chrome-ext-bar" id="xl_chrome_ext_{4DB361DE-01F7-4376-B494-639E489D19ED}" style="display: none;">
      <div class="xl-chrome-ext-bar__logo"></div>

</div>


</body>
</html>